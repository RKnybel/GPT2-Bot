{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ab03ec-bba4-4650-9882-33e6bcd5db22",
   "metadata": {},
   "source": [
    "# GPT2-large Training Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75e3e9-5e68-45d6-9ca2-d8018fc72be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "### PARAMETERS ###\n",
    "config = transformers.GPT2Config.from_pretrained(\"gpt2-large\")\n",
    "config.do_sample = config.task_specific_params['text-generation']['do_sample'] # Ensure random text gen with each prompt\n",
    "config.max_length = config.task_specific_params['text-generation']['max_length'] = 200\n",
    "  \n",
    "print(config)\n",
    "\n",
    "### MODEL AND TOKENIZER ###\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2-large\", config=config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6780f-8cdd-4b09-a834-d7e9b038472d",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "The Tokenizer breaks down a text into words (or tokens) and assigns each an encoding number. The encoding number is used by GPT-2 to identify wordparts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a999e-8cdb-4664-891f-4b0117736474",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer([\"The elf queen\"], return_tensors='pt')\n",
    "print('enc =', enc)\n",
    "print(tokenizer.batch_decode(enc['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9431d-16a6-47a3-93d6-0637ba277fea",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "Use the tokenizer result to prompt the GPT2 model. The tokenizer step encoded the tokens into numbers that the model can understand. The output will need to be decoded for us to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3c924-9bda-4641-bee0-a423cb842fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(input_ids=enc['input_ids'],\n",
    "attention_mask = enc['attention_mask'], max_length = 50)\n",
    "\n",
    "print('out=', out)\n",
    "print(tokenizer.batch_decode(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b46972-b716-4816-9c8c-971a12cdc57c",
   "metadata": {},
   "source": [
    "## Training from the GPT1 Paper Dataset\n",
    "We will make a custom class for the dataset, then use the Transformer library Trainer to do unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509bd24-c10e-44b0-807b-ec6cd34801e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "def break_text_to_pieces(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int = 512) -> list[str]:\n",
    "    \"\"\"Read a file and convert it to tokenized blocks, edding <|endoftext|> to each block\"\"\"\n",
    "    with open(text_path) as f:\n",
    "        text = f.read()\n",
    "    chunk_len0 = block_len - 1  # Leave space for a TOKEN_ENDOFTEXT\n",
    "    tokens = tokenizer.encode(text)\n",
    "    blocks = []\n",
    "    pos = 0\n",
    "    while pos < len(tokens):\n",
    "        chunk = tokens[pos: pos + chunk_len0]\n",
    "        chunk.append(TOKEN_ENDOFTEXT)\n",
    "        blocks.append(chunk)\n",
    "        pos += chunk_len0\n",
    "\n",
    "    if len(blocks[-1]) < block_len:\n",
    "        del blocks[-1]\n",
    "\n",
    "    for block in blocks:\n",
    "        print(len(block))\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def train_val_split(data: list[str], ratio: float):\n",
    "    n = len(data)\n",
    "    assert n >= 2\n",
    "    n_val = max(1, int(n * ratio))\n",
    "    return data[n_val:], data[:n_val]\n",
    "\n",
    "def prepare_dsets(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int):\n",
    "    \"\"\"Read the text, prepare the datasets \"\"\"\n",
    "    data = break_text_to_pieces(text_path, tokenizer, block_len)\n",
    "    data_train, data_val = train_val_split(data, 0.2)\n",
    "    return MyDset(data_train), MyDset(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b006d7-197b-4504-a86a-6fefab52ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class MyDset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: list[list[int]]):\n",
    "        self.data = []\n",
    "        for d in data:\n",
    "            input_ids = torch.tensor(d, dtype=torch.int64)\n",
    "            attention_mask = torch.ones(len(d), dtype=torch.int64)\n",
    "            self.data.append({'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask, 'labels':input_ids})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c2d84-c276-44d3-baec-9dce0f689fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"gpt1_save\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=20,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='no',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31649a98-2cc4-49d6-a36d-e0056c97d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with GPT1\n",
    "TOKEN_ENDOFTEXT = 50256  # <|endoftext|>def break_text_to_pieces(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int = 1024) -> List[List[int]]:\n",
    "    \"\"\"Read a file and convert it to tokenized blocks, adding TOKEN_ENDOFTEXT to each block\"\"\"\n",
    "    with open(text_path) as f:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    blocks = []\n",
    "    pos = 0\n",
    "    while pos < len(tokens):\n",
    "        chunk = tokens[pos: pos + block_len - 1]  # Ensure chunk length is within block_len - 1\n",
    "        if len(chunk) < block_len - 1:\n",
    "            chunk.extend([tokenizer.pad_token_id] * (block_len - 1 - len(chunk)))  # Pad the chunk if needed\n",
    "        chunk.append(tokenizer.eos_token_id)  # Add EOS token\n",
    "        blocks.append(chunk)\n",
    "        pos += len(chunk) - 1  # Move position by the length of the chunk minus EOS token\n",
    "\n",
    "    return blocks\n",
    "BLOCK_LEN = 512\n",
    "TEXT_CORPUS = 'gpt1_paper.txt'\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# model = transformers.GPT2LMHeadModel(transformers.GPT2Config())\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Create datasets and loader\n",
    "dset_train, dset_val = prepare_dsets(TEXT_CORPUS, tokenizer, BLOCK_LEN)\n",
    "print(len(dset_train.__getitem__(0)['input_ids']))\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b030a-60b5-4a4b-8a94-b2dafe29ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised training with reuters dataset turned into corpus\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Get all file IDs from the Reuters corpus\n",
    "file_ids = reuters.fileids()\n",
    "\n",
    "# Initialize an empty list to store the combined text\n",
    "combined_text = []\n",
    "\n",
    "# Iterate through each file ID and read its text\n",
    "for file_id in file_ids:\n",
    "    text = reuters.raw(file_id)\n",
    "    combined_text.append(text)\n",
    "\n",
    "# Combine all text into a single corpus\n",
    "corpus = '\\n'.join(combined_text)\n",
    "\n",
    "# Now you can use the 'corpus' variable for training your model\n",
    "\n",
    "# Write the corpus to a .txt file\n",
    "with open('reuters_corpus.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(corpus)\n",
    "\n",
    "# Train the model with GPT1\n",
    "TOKEN_ENDOFTEXT = 50256  # <|endoftext|>\n",
    "BLOCK_LEN = 1024\n",
    "TEXT_CORPUS = \"reuters_corpus_small.txt\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# model = transformers.GPT2LMHeadModel(transformers.GPT2Config())\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Create datasets and loader\n",
    "dset_train, dset_val = prepare_dsets(TEXT_CORPUS, tokenizer, BLOCK_LEN)\n",
    "for i in range(0,100):\n",
    "    print(len(dset_train.__getitem__(i)['input_ids']))\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f5817-6ad7-4d9d-a512-18928f666b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training\n",
    "model.save_pretrained('./gpt2_reuters_large/')\n",
    "tokenizer.save_pretrained('./gpt2_reuters_large/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda6b76-fc65-4461-ac19-cdfda6145598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_path = \"./gpt2_reuters_large/\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Move model to the same device used for training\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Test loop\n",
    "while True:\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    print('enc =', enc)\n",
    "    print(tokenizer.batch_decode(enc['input_ids']))\n",
    "\n",
    "    out = model.generate(input_ids=enc['input_ids'],\n",
    "                         attention_mask=enc['attention_mask'],\n",
    "                         max_length=50)\n",
    "    \n",
    "    #print('out =', out)\n",
    "    print('Output:', tokenizer.batch_decode(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b374f3e9-f304-4eb2-ab73-685999abcb98",
   "metadata": {},
   "source": [
    "## OpenWebText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f6794c-fde8-49d7-acfd-ce279e832f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "[\"A magazine supplement with an image of Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf” in Germany, but the government of Bavaria, holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitler’s empire is littered with reminders of the Nazi past, from the bullet holes that pit the fronts of many buildings to the hulking Luftwaffe headquarters that now house the Finance Ministry.\\n\\nWhat it doesn’t have, nor has it since 1945, are copies of Hitler’s autobiography and political manifesto, “Mein Kampf,” in its bookstores. The latest attempt to publish excerpts fizzled this week after the Bavarian government challenged it in court, although an expurgated copy appeared at newspaper kiosks around the country.\\n\\nBut in Germany — where keeping a tight lid on Hitler’s writings has become a rich tradition in itself — attitudes toward his book are slowly changing, and fewer people are objecting to its becoming more widely available.\\n\\nNo law bans “Mein Kampf” in Germany, but the government of Bavaria, where Hitler officially resided at the time of his 1945 suicide, holds the copyright and guards it ferociously. German-language copies that were printed before 1945 are legal, although they command a premium price, and the book is available in translation elsewhere in the world.\\n\\nBut the question of whether to publish it in the country where Hitler plotted his empire has lost some of its edge in the Google era, when a complete German-language copy of the book pops up as the second result on the local version of the search engine.\\n\\n“To say this is a very dangerous book, we must ban it, this is ridiculous,” said Wolfgang Wippermann, a professor of modern history at the Free University of Berlin. “Maybe it was necessary once, but now it’s over, it makes no sense. You can find it so easily.”\\n\\nThe publisher of the excerpts, London-based Albertus, has said it will appeal the Bavarian government’s injunction. In 2009, the publisher beat charges of copyright violation and the illegal use of Nazi symbols after the Bavarian government seized reprinted copies of the Nazi Party’s in-house newspaper.\\n\\nThe attempt to publish portions of “Mein Kampf” on Thursday was scuttled at the last moment, although the publisher, ready to capitalize on the publicity, had printed two versions of the pamphlet. The version propped on top of a heap of celebrity magazines at a newsstand in Berlin’s central Friedrichstrasse station was a slender, blue, 16-page leaflet that has historical commentary in one column and an image of blurred text stamped with “Unreadable” in the other, accompanied by two reproductions of Nazi-era newspapers.\\n\\n“Mein Kampf” “is an awful book, and the whole thinking is absolutely not ours, but we have another view on it regarding the idea of packing it away. This idea is just naive,” said Alexander Luckow, a spokesman for the publisher. “In a free country, you need to discuss these very bad parts of German history.”\\n\\nStill, he said, there are limits, and using Hitler’s words as inspiration, not as historical artifact, is where it crosses the line.\\n\\n“The danger is allowing right-wing people to sell it in bookshops with their modern commentary,” he said. “This is forbidden and it’s good . . . not only in Germany, this should be equal in other countries in Europe. Anti-Semitism is not confined to Germany. You look and it’s all around Europe, dating back to the Middle Ages.”\\n\\nThe debate will soon be over, whether or not the latest excerpts make it to newsstands. German law extends copyright 70 years after the author’s death; after 2015, “Mein Kampf” will be fair game. Some in Bavaria’s government worry that neo-Nazis will publish their own version of the book shortly thereafter, and to counter that, they are encouraging a scholarly edition. A group of historians is preparing it.\\n\\nGermany’s Jewish organizations have approached the publication with mixed emotions, sensitive that their country still has problems with neo-Nazis and anti-Semitism. The German government released a study this week that found that one in five Germans has anti-Semitic attitudes. And a neo-Nazi ring that has been linked to at least nine killings before it was shut down in November shocked Germans who thought they had done a thorough job working through their past.\\n\\n“I do very well without any publishing of ‘Mein Kampf,’ ” said Dieter Graumann, the head of the Central Council of Jews in Germany. “In a few years, it will be free, and I have every trust in the democratic maturity of the German people. . . . But for the moment, I am glad it is not.”\"]\n"
     ]
    }
   ],
   "source": [
    "# Download and format the dataset into a corpus of text\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the OpenWebText-10k dataset\n",
    "openwebtext_dataset = load_dataset(\"stas/openwebtext-10k\")\n",
    "\n",
    "# Print information about the dataset\n",
    "print(openwebtext_dataset)\n",
    "print(openwebtext_dataset[\"train\"])\n",
    "print(openwebtext_dataset['train'][\"text\"][:1])\n",
    "# Write the corpus to a .txt file\n",
    "with open(\"openwebtext-10k.txt\", 'w', encoding='utf-8') as file:\n",
    "    for text in openwebtext_dataset['train']['text']:\n",
    "        file.write(text + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fbf549-865e-40d2-86e0-8899e1918527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "def break_text_to_pieces(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int = 512) -> list[str]:\n",
    "    \"\"\"Read a file and convert it to tokenized blocks, edding <|endoftext|> to each block\"\"\"\n",
    "    with open(text_path) as f:\n",
    "        text = f.read()\n",
    "    chunk_len0 = block_len - 1  # Leave space for a TOKEN_ENDOFTEXT\n",
    "    tokens = tokenizer.encode(text)\n",
    "    blocks = []\n",
    "    pos = 0\n",
    "    while pos < len(tokens):\n",
    "        chunk = tokens[pos: pos + chunk_len0]\n",
    "        chunk.append(TOKEN_ENDOFTEXT)\n",
    "        blocks.append(chunk)\n",
    "        pos += chunk_len0\n",
    "\n",
    "    if len(blocks[-1]) < block_len:\n",
    "        del blocks[-1]\n",
    "\n",
    "    #for block in blocks:\n",
    "    #    print(len(block))\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def train_val_split(data: list[str], ratio: float):\n",
    "    n = len(data)\n",
    "    assert n >= 2\n",
    "    n_val = max(1, int(n * ratio))\n",
    "    return data[n_val:], data[:n_val]\n",
    "\n",
    "def prepare_dsets(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int):\n",
    "    \"\"\"Read the text, prepare the datasets \"\"\"\n",
    "    data = break_text_to_pieces(text_path, tokenizer, block_len)\n",
    "    data_train, data_val = train_val_split(data, 0.2)\n",
    "    return MyDset(data_train), MyDset(data_val)\n",
    "\n",
    "# Custom Dataset\n",
    "class MyDset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: list[list[int]]):\n",
    "        self.data = []\n",
    "        for d in data:\n",
    "            input_ids = torch.tensor(d, dtype=torch.int64)\n",
    "            attention_mask = torch.ones(len(d), dtype=torch.int64)\n",
    "            self.data.append({'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask, 'labels':input_ids})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bccddb-61c7-4254-8e67-55f786670140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1953143 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1314' max='30560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1314/30560 04:33 < 1:41:46, 4.79 it/s, Epoch 0.86/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training configuration\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"gpt1_save\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=20,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    fp16=True,  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "# Train the model with openwebtext-10k\n",
    "\n",
    "TOKEN_ENDOFTEXT = 50256  # <|endoftext|>\n",
    "BLOCK_LEN = 1024\n",
    "TEXT_CORPUS = \"openwebtext-10k-small.txt\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# model = transformers.GPT2LMHeadModel(transformers.GPT2Config())\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Create datasets and loader\n",
    "dset_train, dset_val = prepare_dsets(TEXT_CORPUS, tokenizer, BLOCK_LEN)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d42078-4677-43fa-93ac-3278d48c7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training\n",
    "model.save_pretrained('./gpt2_openwebtext10k_large/')\n",
    "tokenizer.save_pretrained('./gpt2_openwebtext10k_large/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
