{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4984bc64-5bb9-479f-9695-8fcfff79aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riley/miniconda3/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/riley/miniconda3/envs/ml/lib/python3.10/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "import transformers\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# CUDA gradient accumulation\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2_torch_openwebtext-10k\")\n",
    "# model = transformers.GPT2LMHeadModel(transformers.GPT2Config())\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2_torch_openwebtext-10k\")\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "def break_text_to_pieces(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int = 512) -> list[str]:\n",
    "    \"\"\"Read a file and convert it to tokenized blocks, edding <|endoftext|> to each block\"\"\"\n",
    "    with open(text_path) as f:\n",
    "        text = f.read()\n",
    "    chunk_len0 = block_len - 1  # Leave space for a TOKEN_ENDOFTEXT\n",
    "    tokens = tokenizer.encode(text)\n",
    "    blocks = []\n",
    "    pos = 0\n",
    "    while pos < len(tokens):\n",
    "        chunk = tokens[pos: pos + chunk_len0]\n",
    "        chunk.append(TOKEN_ENDOFTEXT)\n",
    "        blocks.append(chunk)\n",
    "        pos += chunk_len0\n",
    "\n",
    "    if len(blocks[-1]) < block_len:\n",
    "        del blocks[-1]\n",
    "\n",
    "    #for block in blocks:\n",
    "    #    print(len(block))\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def train_val_split(data: list[str], ratio: float):\n",
    "    n = len(data)\n",
    "    assert n >= 2\n",
    "    n_val = max(1, int(n * ratio))\n",
    "    return data[n_val:], data[:n_val]\n",
    "\n",
    "def prepare_dsets(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int):\n",
    "    \"\"\"Read the text, prepare the datasets \"\"\"\n",
    "    data = break_text_to_pieces(text_path, tokenizer, block_len)\n",
    "    data_train, data_val = train_val_split(data, 0.2)\n",
    "    return MyDset(data_train), MyDset(data_val)\n",
    "\n",
    "# Custom Dataset\n",
    "class MyDset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: list[list[int]]):\n",
    "        self.data = []\n",
    "        for d in data:\n",
    "            input_ids = torch.tensor(d, dtype=torch.int64)\n",
    "            attention_mask = torch.ones(len(d), dtype=torch.int64)\n",
    "            self.data.append({'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask, 'labels':input_ids})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.data[idx]\n",
    "\n",
    "## How one epoch of training happens\n",
    "def train_one(model, loader, optimizer):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in tqdm.tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(DEVICE)\n",
    "            out = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "            loss = out['loss']\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "\n",
    "## How one epoch of validation happens \n",
    "def val_one(model: torch.nn.Module, loader: torch.utils.data.DataLoader):\n",
    "    \"\"\"Standard PyTorch eval, one epoch\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in tqdm.tqdm(loader):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "        # loss, logits, past_key_values\n",
    "        loss = out['loss']\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff19d9a-23b9-4255-838d-6c0474da45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "\n",
    "\n",
    "TOKEN_ENDOFTEXT = 50256  # <|endoftext|>\n",
    "BLOCK_LEN = 1024\n",
    "TEXT_CORPUS = \"dialogs_processed.txt\"\n",
    "model.to(DEVICE)\n",
    "# Create datasets and loader\n",
    "dset_train, dset_val = prepare_dsets(TEXT_CORPUS, tokenizer, BLOCK_LEN)\n",
    "loader_train = torch.utils.data.DataLoader(dset_train, batch_size=1)\n",
    "loader_val = torch.utils.data.DataLoader(dset_val, batch_size=1)\n",
    "\n",
    "# Optimizer, device\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for i_epoch in range(1):\n",
    "    loss_train = train_one(model, loader_train, optimizer)\n",
    "    loss_val = val_one(model, loader_val)\n",
    "    print(f'{i_epoch} : loss_train={loss_train}, loss_val={loss_val}')\n",
    "\n",
    "\n",
    "# Save the model if needed\n",
    "if True:\n",
    "    model.save_pretrained('./gpt2_torch_conversational_openwebtext-10k/')\n",
    "    tokenizer.save_pretrained('./gpt2_torch_conversational_openwebtext-10k/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad430097-ed31-402f-87f5-8d87d3a8bee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mGPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2_torch_conversational_openwebtext-10k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are Bot. User will ask you questions. Please do your best to respond to the user helpfully. Keep your response inside one line, following \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/transformers/modeling_utils.py:2692\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2691\u001b[0m         )\n\u001b[0;32m-> 2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "# Now our model is trained, try the generation\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2_torch_conversational_openwebtext-10k\")\n",
    "history = \"You are Bot. User will ask you questions. Please do your best to respond to the user helpfully. Keep your response inside one line, following \\\"Bot: \\\"\\n\\n\"\n",
    "model.to(DEVICE)\n",
    "while True:\n",
    "    text = input(\"User: \")\n",
    "    history += \"User: \" + text\n",
    "    batch = tokenizer([history], return_tensors='pt')\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(DEVICE)\n",
    "    out = model.generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=1000, temperature=1.8, top_k=30, do_sample=True )\n",
    "    out = tokenizer.batch_decode(out.cpu())[0]\n",
    "    input_length = len(history)\n",
    "    history += \"\\nBot: \" + out[input_length:input_length+100].split(\"User: \")[0] + \"\\n\" #only save 100 characters of context from the bot\n",
    "    print(out[input_length:].split(\"User: \")[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf2e04-950e-4269-8e3f-cfdc92a27206",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b5fcbb-fe23-43b3-a882-c4878ee5fc9f",
   "metadata": {},
   "source": [
    "You are Bot. User will ask you questions. Please do your best to respond to the user helpfully.\n",
    "\n",
    "User: Hi buddy\n",
    "Bot: , i have a good job.\n",
    "Bot: well, i'm not a doctor.\n",
    "\n",
    "User: You can be a doctor\n",
    "Bot: \n",
    "Bot: are you doing so?\n",
    "\n",
    "User: Well, no. I am a software developer. But you can be whatever you want to be.\n",
    "Bot: Bot: i like to do that.\n",
    "\n",
    "User: I bet. Do you like music?\n",
    "Bot: Bot: what's the matter?\n",
    "\n",
    "User: Nothing, just asking if you like music.\n",
    "Bot: Bot: well, that's not a nice job.\n",
    "\n",
    "User: I'm sorry.\n",
    "Bot: Bot: you're not a doctor.\n",
    "\n",
    "User: That much is clear. You're not a doctor either.\n",
    "Bot: Bot: you'll have to walk out over the world.\n",
    "\n",
    "User: No, you.\n",
    "Bot: you'll have to walk out over the world.\n",
    "Bot: you need to do that.\n",
    "\n",
    "User: No. You.\n",
    "Bot: you need to do that.\n",
    "Bot: you need to walk out over the world.\n",
    "\n",
    "User: Ok fine, maybe I will walk out over the world. Are you happy now?\n",
    "Bot: Bot: i want to do that.\n",
    "\n",
    "User: Then do so!\n",
    "Bot: Bot: then do you know what?\n",
    "\n",
    "User: What?\n",
    "Bot: Bot: i know i was a doctor from the beginning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
